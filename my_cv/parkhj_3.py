import cv2
import numpy as np
import pyrealsense2 as rs
import easyocr
import requests
import time
import threading
from PIL import Image, ImageDraw, ImageFont
from queue import Queue, Empty

# =====================================
# 1. DeepL
# =====================================
DEEPL_API_KEY = "1e8f1e6d-24b6-498f-9cc4-0d0ad9f8110d:fx"

CANNY_LOW  = 50
CANNY_HIGH = 150
MIN_AREA = 8000
MAX_AREA = 200000
OCR_INTERVAL = 0.5
ALPHA = 0.3

missing_frame_count = 0 
MAX_MISSING_FRAMES = 5

# ë²ˆì—­ ìš”ì²­/ìºì‹œ
translate_queue = Queue()
translation_cache = {}      # { full_eng(str): kor(str) }
requested_texts  = set()    # ì´ë¯¸ ìš”ì²­ ë„£ì€ ì˜ì–´ ë¬¸ì¥ë“¤


# =====================================
# DeepL ë²ˆì—­ ìŠ¤ë ˆë“œ
# =====================================
def translate(text_eng: str) -> str:
    if not text_eng.strip():
        return "(í…ìŠ¤íŠ¸ ì—†ìŒ)"

    url = "https://api-free.deepl.com/v2/translate"
    headers = {"Authorization": f"DeepL-Auth-Key {DEEPL_API_KEY}"}
    data = {"text": text_eng, "source_lang": "EN", "target_lang": "KO"}

    try:
        # ğŸ”¥ íƒ€ì„ì•„ì›ƒì„ 5.0ì´ˆë¡œ ìœ ì§€
        r = requests.post(url, headers=headers, data=data, timeout=5.0) 
        r.raise_for_status()
        j = r.json()
        t = j.get("translations", [])
        if not t:
            # DeepLì—ì„œ ì •ìƒ ì‘ë‹µí–ˆìœ¼ë‚˜ ë²ˆì—­ ê²°ê³¼ê°€ ì—†ì„ ê²½ìš°
            print(f"[DeepL ERROR] No translation found for: {text_eng[:30]}...")
            return "(ë²ˆì—­ ê²°ê³¼ ì—†ìŒ)"
        return t[0].get("text", "(ë²ˆì—­ ì˜¤ë¥˜)")
    except requests.exceptions.RequestException as e:
        # ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜, íƒ€ì„ì•„ì›ƒ, HTTP ì˜¤ë¥˜(4xx, 5xx) ë“±
        print(f"[DeepL API ERROR] Request failed for '{text_eng[:30]}...': {e}")
        return "(ë²ˆì—­ ì˜¤ë¥˜)"
    except Exception as e:
        # ê¸°íƒ€ JSON íŒŒì‹± ì˜¤ë¥˜ ë“±
        print(f"[DeepL API ERROR] An unexpected error occurred: {e}")
        return "(ë²ˆì—­ ì˜¤ë¥˜)"


def translate_thread():
    global translation_cache
    while True:
        try:
            # íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
            text_eng = translate_queue.get(timeout=1)
        except Empty:
            continue
        
        # ë²ˆì—­ ì‹œë„
        trans = translate(text_eng)
        
        # ë²ˆì—­ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
        translation_cache[text_eng] = trans

        # í ì‘ì—… ì™„ë£Œ ë³´ê³ 
        translate_queue.task_done()


# í ì‘ì—… ìŠ¤ë ˆë“œ ì‹œì‘
threading.Thread(target=translate_thread, daemon=True).start()


# =====================================
# ì‚¬ê°í˜• warp í•¨ìˆ˜
# =====================================
def order_points(pts):
    rect = np.zeros((4, 2), dtype="float32")
    s = pts.sum(axis=1)
    diff = np.diff(pts, axis=1)

    rect[0] = pts[np.argmin(s)]
    rect[2] = pts[np.argmax(s)]
    rect[1] = pts[np.argmin(diff)]
    rect[3] = pts[np.argmax(diff)]
    return rect


def warp_quad(image, quad):
    pts = quad.reshape(4, 2).astype("float32")
    rect = order_points(pts)

    (tl, tr, br, bl) = rect
    widthA = np.linalg.norm(br - bl)
    widthB = np.linalg.norm(tr - tl)
    maxW = int(max(widthA, widthB))

    heightA = np.linalg.norm(tr - br)
    heightB = np.linalg.norm(tl - bl)
    maxH = int(max(heightA, heightB))
    maxW = max(maxW, 40)
    maxH = max(maxH, 40)

    dst = np.array([[0,0],[maxW-1,0],[maxW-1,maxH-1],[0,maxH-1]], dtype="float32")
    M = cv2.getPerspectiveTransform(rect, dst)
    warped = cv2.warpPerspective(image, M, (maxW, maxH))

    return warped, M, rect, dst


# =====================================
# ğŸ”¥ ë¬¸ì¥ ë‹¨ìœ„ í´ëŸ¬ìŠ¤í„°ë§
# =====================================
def group_words(results):
    if len(results) == 0:
        return []

    words = []
    for r in results:
        bbox, text, _ = r
        pts = np.array(bbox)

        # bbox ìƒë‹¨ y í‰ê· 
        top_y = int((pts[0][1] + pts[1][1]) / 2)
        # ì¢Œìš° ìˆœì„œ ì•ˆì •í™” â†’ ìµœì†Œ x
        min_x = int(np.min(pts[:,0]))

        words.append((min_x, top_y, bbox, text))

    # ì¤„ ìˆœìœ¼ë¡œ y ì •ë ¬
    words.sort(key=lambda x: x[1])

    clusters = []
    current = [words[0]]

    LINE_GAP = 20
    WORD_GAP = 130

    for i in range(1, len(words)):
        prev = current[-1]
        curr = words[i]

        # yì¶• ì°¨ì´ê°€ ì‘ìœ¼ë©´ ê°™ì€ ì¤„
        if abs(prev[1] - curr[1]) < LINE_GAP:
            # xì¶• ì°¨ì´ê°€ ì‘ìœ¼ë©´ ê°™ì€ ë¬¸ì¥
            if abs(prev[0] - curr[0]) < WORD_GAP:
                current.append(curr)
            else:
                clusters.append(current)
                current = [curr]
        else:
            clusters.append(current)
            current = [curr]

    clusters.append(current)

    # ê° í´ëŸ¬ìŠ¤í„° ë‚´ x ì •ë ¬
    for c in clusters:
        c.sort(key=lambda x: x[0])

    return clusters


# =====================================
# overlay ìƒì„± (warp ê³µê°„)
# =====================================
def create_overlay(h, w, word_states, warped_img):
    """
    word_states: { key: {"eng":..., "kor":..., "bbox":[[x,y],...]} }
    warped_img: í˜„ì¬ AR ì²˜ë¦¬ê°€ ì§„í–‰ ì¤‘ì¸ ì›ë³¸ ì˜ì—­ (warped) ì´ë¯¸ì§€
    """
    # [AR ë°°ê²½] í°ìƒ‰ ë°˜íˆ¬ëª… ì˜¤ë²„ë ˆì´ë¥¼ ìœ„í•´ np.ones * 255 ì‚¬ìš© (ì‚¬ìš©ì ì„¤ì • ìœ ì§€)
    overlay = np.ones((h, w, 3), dtype=np.uint8) * 255
    pil_img = Image.fromarray(overlay)
    draw = ImageDraw.Draw(pil_img)

    for key, st in word_states.items():
        kor = st["kor"]
        bbox = np.array(st["bbox"])

        # bbox ì˜ì—­
        min_x = int(np.min(bbox[:,0]))
        max_x = int(np.max(bbox[:,0]))
        min_y = int(np.min(bbox[:,1]))
        max_y = int(np.max(bbox[:,1]))
        
        # ğŸ”¥ 1. í…ìŠ¤íŠ¸ BBOXì˜ ì›ë³¸ ì´ë¯¸ì§€ í”½ì…€ í‰ê·  ë°ê¸° ê³„ì‚° (warped_img ì‚¬ìš©)
        # ------------------------------------------------------------------
        text_roi = warped_img[min_y:max_y, min_x:max_x] 

        # í…ìŠ¤íŠ¸ ì˜ì—­ì˜ í‰ê·  ë°ê¸° (grayscale) ê³„ì‚°
        if text_roi.size > 0:
            # BGRì„ Grayë¡œ ë³€í™˜ í›„ í‰ê·  ê³„ì‚°
            gray_roi = cv2.cvtColor(text_roi, cv2.COLOR_BGR2GRAY)
            avg_brightness = np.mean(gray_roi)
        else:
            # ROIê°€ ë„ˆë¬´ ì‘ê±°ë‚˜ ì—†ì„ ê²½ìš° ëŒ€ë¹„ (ì¤‘ê°„ ë°ê¸°)
            avg_brightness = 128
            
        # 2. ë°ê¸°ì— ë”°ë¼ í…ìŠ¤íŠ¸ ìƒ‰ìƒ ê²°ì • (ê³ ëŒ€ë¹„)
        # ì„ê³„ê°’ 30: ì‚¬ìš©ìì˜ í”¼ë“œë°±ì— ë”°ë¼ ì„ê³„ê°’ì„ 30ìœ¼ë¡œ ì„¤ì •í•˜ì—¬, 
        #           ë§¤ìš° ì–´ë‘ìš´ ë°°ê²½(ë°ê¸° <= 30)ì´ ì•„ë‹ˆë©´ í°ìƒ‰ í…ìŠ¤íŠ¸ë¥¼ ì ê·¹ì ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        CONTRAST_THRESHOLD = 30 
        if avg_brightness > CONTRAST_THRESHOLD:
            text_color = (0, 0, 0)   # Black (ê²€ì€ìƒ‰)
        else:
            text_color = (255, 255, 255) # White (í°ìƒ‰)
        # ------------------------------------------------------------------

        # OCR ê¸€ì”¨ ë†’ì´
        height = max_y - min_y
        if height < 5:
            height = 5

        # 1) ë²ˆì—­ ê¸€ì”¨ í¬ê¸° (ê¸€ì”¨ ë†’ì´ì— ë¹„ë¡€)
        font_size = int(height * 0.5)
        font_size = max(font_size, 10) # ìµœì†Œ í°íŠ¸ í¬ê¸° 10pt ë³´ì¥
        
        try:
            # NotoSansCJK-Regular.ttc í°íŠ¸ ì‚¬ìš©
            font = ImageFont.truetype(
                "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
                font_size
            )
        except:
            font = ImageFont.load_default()

        # 2) ë²ˆì—­ ê¸€ì”¨ ìœ„ì¹˜ (ê¸€ì”¨ ì•„ë˜ + ë†’ì´ * 0.7)
        text_y = min(max_y + int(height * 0.5), h - height) 

        text = kor
        tb = draw.textbbox((0,0), text, font=font)
        tw = tb[2] - tb[0]

        text_x = min_x + (max_x - min_x)//2 - tw//2

        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°: ê³„ì‚°ëœ text_color ì‚¬ìš©
        draw.text((text_x, text_y), text, fill=text_color, font=font)

    return np.array(pil_img)

# main í•¨ìˆ˜ ë°–ì— ì •ì˜ (í˜¹ì€ main í•¨ìˆ˜ ì•ˆì— nested functionìœ¼ë¡œ ì •ì˜)
def select_and_translate_roi(frame, reader):
    """
    í”„ë ˆì„ì—ì„œ ROIë¥¼ ì„ íƒí•˜ê³ , í•´ë‹¹ ì˜ì—­ì˜ í…ìŠ¤íŠ¸ë¥¼ OCR ë° ë²ˆì—­ í›„ íŒì—…ìœ¼ë¡œ í‘œì‹œ
    (ìˆ˜ì •: ì›ë³¸ í…ìŠ¤íŠ¸ ìœ„ì¹˜ì— ë²ˆì—­ í…ìŠ¤íŠ¸ë¥¼ ì˜¤ë²„ë ˆì´í•©ë‹ˆë‹¤.)
    """
    # 1. ì›ë³¸ í”„ë ˆì„ ë³µì‚¬
    temp_frame = frame.copy()
    
    # 2. ROI ì„ íƒ ì°½ í‘œì‹œ
    win_name = "Select ROI for Translation"
    cv2.namedWindow(win_name)
    
    # ì„ íƒ ì˜ì—­ ì¢Œí‘œ (x, y, w, h)
    ret = cv2.selectROI(win_name, temp_frame)
    cv2.destroyWindow(win_name) # ROI ì„ íƒ í›„ ì°½ ë‹«ê¸°
    
    x, y, w, h = ret
    
    if w > 0 and h > 0:
        # 3. ROI ì˜ì—­ ì˜ë¼ë‚´ê¸°
        roi_img = frame[int(y):int(y+h), int(x):int(x+w)]
        
        # 4. OCR ìˆ˜í–‰ (ì›ë³¸ í¬ê¸° ì´ë¯¸ì§€ ì‚¬ìš©)
        print("[ROI OCR] Running OCR on selected area...")
        roi_gray = cv2.cvtColor(roi_img, cv2.COLOR_BGR2GRAY)
        
        # EasyOCR ì‹¤í–‰ (WARP ì—†ì´ ë°”ë¡œ ì‹¤í–‰)
        # ê²°ê³¼: [[bbox, text, confidence], ...]
        results = reader.readtext(roi_gray, detail=1) 
        
        if not results:
            print("[ROI OCR] No text found in ROI.")
            return

        # 5. í…ìŠ¤íŠ¸ ê·¸ë£¹í™” ë° ë²ˆì—­ ìš”ì²­
        clusters = group_words(results) # ê¸°ì¡´ í•¨ìˆ˜ í™œìš©
        
        # 5-1. í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ì˜ì–´ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ê³  DeepL ìš”ì²­ì„ ìœ„í•œ í…ìŠ¤íŠ¸ë¥¼ ë§Œë“¦
        all_eng_sentences = []
        for cluster in clusters:
            full_eng = " ".join([word[3] for word in cluster]).strip()
            if full_eng:
                all_eng_sentences.append(full_eng)
                
        full_eng_text = "\n".join(all_eng_sentences)

        # 6. ë™ê¸° ë²ˆì—­ (DeepL ìš”ì²­) - ë™ê¸° í˜¸ì¶œì€ ì˜¤ë¥˜ ë°œìƒ ì‹œ íŒì—…ì— ì¦‰ì‹œ ë°˜ì˜ë¨
        if not full_eng_text:
            kor_translation = "(í…ìŠ¤íŠ¸ ì—†ìŒ)"
        else:
            print("[ROI OCR] Found: '{}'".format(full_eng_text.replace('\n', ' ')))
            kor_translation = translate(full_eng_text)
        
        # 6-1. ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ì™€ ë§¤í•‘ ì¤€ë¹„
        kor_lines = kor_translation.split('\n')
        
        # 7. ë²ˆì—­ ê²°ê³¼ íŒì—… í‘œì‹œ
        
        # --- 7-1. ì´ë¯¸ì§€ í™•ëŒ€ ê³„ì‚° ë° ì ìš© ---
        h_orig, w_orig = roi_img.shape[:2]
        MIN_DIMENSION = 360 # ìµœì†Œ 200í”½ì…€ ë³´ì¥
        
        scale = 1.0
        
        # ì›ë³¸ ROI ì´ë¯¸ì§€ì˜ í¬ê¸°ê°€ ìµœì†Œ í¬ê¸°(200)ë³´ë‹¤ ì‘ì„ ê²½ìš°ì—ë§Œ í™•ëŒ€
        if w_orig < MIN_DIMENSION or h_orig < MIN_DIMENSION:
            scale_w = MIN_DIMENSION / w_orig
            scale_h = MIN_DIMENSION / h_orig
            scale = max(scale_w, scale_h) 

            new_w = int(w_orig * scale)
            new_h = int(h_orig * scale)
            
            resized_roi_img = cv2.resize(roi_img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
        else:
            resized_roi_img = roi_img.copy()
        
        # 7-2. ì˜¤ë²„ë ˆì´ í•©ì„± ì¤€ë¹„ (í™•ëŒ€ëœ ì´ë¯¸ì§€ ê¸°ë°˜)
        pil_roi_img = Image.fromarray(cv2.cvtColor(resized_roi_img, cv2.COLOR_BGR2RGB)).convert('RGBA')

        # ğŸ”¥ [í•µì‹¬ ìˆ˜ì •] ë°˜íˆ¬ëª… ë°°ê²½ê³¼ í…ìŠ¤íŠ¸ë¥¼ ê·¸ë¦´ íˆ¬ëª… ë ˆì´ì–´ ìƒì„±
        overlay_layer = Image.new('RGBA', pil_roi_img.size, (0, 0, 0, 0))
        draw = ImageDraw.Draw(overlay_layer)
        
        TRANSPARENCY_BKG = 180 # 180/255 = ì•½ 70% ë¶ˆíˆ¬ëª… (ì‚¬ìš©ìê°€ 80ì„ ì‹œë„í–ˆìœ¼ë¯€ë¡œ 180ì€ ëˆˆì— ë„ê²Œ ë°˜íˆ¬ëª…í•¨)
        PADDING = 2            # í…ìŠ¤íŠ¸ ì˜ë¦¼ ë°©ì§€ë¥¼ ìœ„í•´ ë°°ê²½ ë°•ìŠ¤ì— ì—¬ë°± ì¶”ê°€

        # í´ëŸ¬ìŠ¤í„° ìˆœíšŒ ë° ì˜¤ë²„ë ˆì´
        for i, cluster in enumerate(clusters):
            # ë¬¸ì¥ë³„ ë²ˆì—­ ê²°ê³¼ ë§¤í•‘ (ë²ˆì—­ ì˜¤ë¥˜ ì‹œì—ë„ í•´ë‹¹ ì˜¤ë¥˜ ë©”ì‹œì§€ ì‚¬ìš©)
            kor_text = kor_lines[i] if i < len(kor_lines) else "(ë²ˆì—­ ì˜¤ë¥˜)"
            
            # 1. ë¬¸ì¥ ì „ì²´ BBox ê³„ì‚° (ì›ë³¸ ROI ì´ë¯¸ì§€ ê¸°ì¤€ ì¢Œí‘œ)
            all_pts = []
            for _, _, bbox, _ in cluster:
                all_pts.extend(bbox)
            all_pts = np.array(all_pts)
            
            orig_min_x = int(np.min(all_pts[:, 0]))
            orig_max_x = int(np.max(all_pts[:, 0]))
            orig_min_y = int(np.min(all_pts[:, 1]))
            orig_max_y = int(np.max(all_pts[:, 1]))
            
            # 2. í™•ëŒ€ ë¹„ìœ¨ ì ìš©í•˜ì—¬ í™•ëŒ€ëœ ì´ë¯¸ì§€ ìœ„ì—ì„œì˜ BBox ì¢Œí‘œ ê³„ì‚°
            scaled_min_x = int(orig_min_x * scale)
            scaled_max_x = int(orig_max_x * scale)
            scaled_min_y = int(orig_min_y * scale)
            scaled_max_y = int(orig_max_y * scale)
            
            # 3. ë°˜íˆ¬ëª… ë°°ê²½ (OCR BBox ì˜ì—­ì„ ë®ìŒ) - PADDING ì ìš©
            # ë°°ê²½ì€ ë³„ë„ì˜ íˆ¬ëª… ë ˆì´ì–´(overlay_layer)ì— ê·¸ë¦½ë‹ˆë‹¤.
            draw.rectangle([
                (scaled_min_x - PADDING, scaled_min_y - PADDING), 
                (scaled_max_x + PADDING, scaled_max_y + PADDING)
            ], fill=(0, 0, 0, TRANSPARENCY_BKG)) 
            
            # 4. í…ìŠ¤íŠ¸ í°íŠ¸ ì„¤ì •
            bbox_height = scaled_max_y - scaled_min_y
            current_font_size = int(bbox_height * 0.4) 
            current_font_size = max(current_font_size, 15)
            
            try:
                font = ImageFont.truetype(
                    "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
                    current_font_size
                )
            except:
                font = ImageFont.load_default()

            # 5. í…ìŠ¤íŠ¸ ìœ„ì¹˜ ê³„ì‚° (BBox ì¤‘ì•™)
            tb = draw.textbbox((0, 0), kor_text, font=font)
            tw = tb[2] - tb[0]
            th = tb[3] - tb[1]
            
            # BBox ì¤‘ì•™ì— í…ìŠ¤íŠ¸ ìœ„ì¹˜
            text_pos_x = scaled_min_x + (scaled_max_x - scaled_min_x) // 2 - tw // 2
            text_pos_y = scaled_min_y + (scaled_max_y - scaled_min_y) // 2 - th // 2
            
            # 6. í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (í°ìƒ‰ìœ¼ë¡œ ê³ ì •)
            # í…ìŠ¤íŠ¸ë„ ë³„ë„ì˜ íˆ¬ëª… ë ˆì´ì–´(overlay_layer)ì— ê·¸ë¦½ë‹ˆë‹¤.
            draw.text((text_pos_x, text_pos_y), kor_text, fill=(255, 255, 255), font=font)

        # ğŸ”¥ 7-4. ì›ë³¸ ì´ë¯¸ì§€ì™€ ë°˜íˆ¬ëª… ë°°ê²½/í…ìŠ¤íŠ¸ ë ˆì´ì–´ë¥¼ í•©ì„±í•©ë‹ˆë‹¤.
        pil_roi_img = Image.alpha_composite(pil_roi_img, overlay_layer)
        
        # 7-5. PIL ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (RGBA->BGR)
        result_img_cv = cv2.cvtColor(np.array(pil_roi_img), cv2.COLOR_RGBA2BGR)
        # ----------------------------------------------------

        # 7-6. ê²°ê³¼ ì°½ í‘œì‹œ ë° ëŒ€ê¸°
        cv2.namedWindow("ROI Translation Result", cv2.WINDOW_AUTOSIZE)
        cv2.imshow("ROI Translation Result", result_img_cv)
        
        # [í•µì‹¬] ì‚¬ìš©ìê°€ ë‹«ì„ ë•Œê¹Œì§€ ëŒ€ê¸° (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ë©”ì¸ ì˜ìƒì€ ì•ˆ ë©ˆì¶¤)
        cv2.waitKey(0) 
        
        # ì°½ ë‹«ê¸°
        cv2.destroyWindow("ROI Translation Result")
        cv2.destroyWindow(win_name)

# =====================================
# 5. ë©”ì¸ (ì—¬ëŸ¬ ì¥ A4 ì§€ì›) [ìˆ˜ì •ë¨]
# =====================================
def main():
    global missing_frame_count, MAX_MISSING_FRAMES
    print("Initializing EasyOCR...")
    reader = easyocr.Reader(['en'])

    pipeline = rs.pipeline()
    config = rs.config()
    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
    pipeline.start(config)
    print("RealSense started.")

    last_overlay = None
    last_mask = None
    last_ocr_time = 0.0

    while True:
        frames = pipeline.wait_for_frames()
        f = frames.get_color_frame()
        if not f:
            continue

        frame = np.asanyarray(f.get_data())
        cv2.imshow("Input", frame)
        output = frame.copy()
        h, w = frame.shape[:2]
        now = time.time()

        # ----- Canny -----
        g = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        # cv2.imshow("Gray", g)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        g = clahe.apply(g)
        # cv2.imshow("CLAHE", g)
        g_blur = cv2.GaussianBlur(g, (5,5), 0)
        # cv2.imshow("Blurred", g_blur)
        edges = cv2.Canny(g_blur, CANNY_LOW, CANNY_HIGH)
        # cv2.imshow("Canny Edges", edges)
        kernel = np.ones((3,3), np.uint8)
        edges = cv2.dilate(edges, kernel, 1)
        edges = cv2.erode(edges, kernel, 1)
        # cv2.imshow("Morphology", edges)

        # ----- contour â†’ ì—¬ëŸ¬ ê°œ quad í›„ë³´ -----
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        quads = []
        output2 = output.copy()
        cv2.drawContours(output2, contours, -1, (255,0,0), 2)
        # cv2.imshow("All Contours", output2)

        for cnt in contours:
            peri = cv2.arcLength(cnt, True)
            approx = cv2.approxPolyDP(cnt, 0.08*peri, True)

            if len(approx) == 4:
                if not cv2.isContourConvex(approx):
                    continue
                area = cv2.contourArea(approx)
                if area < MIN_AREA or area > MAX_AREA:
                    continue
                hull = cv2.convexHull(cnt) # ì›ë˜ ì»¨íˆ¬ì–´ë¡œ ì»¨ë²¡ìŠ¤ í— ê³„ì‚°
                hull_area = cv2.contourArea(hull)
                if hull_area == 0: # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì˜¤ë¥˜ ë°©ì§€
                    continue
                solidity = float(area) / hull_area
                if solidity < 0.8: # 80% ë¯¸ë§Œì´ë©´ ì œì™¸
                    continue
                x,y,w2,h2 = cv2.boundingRect(approx)
                asp = w2/float(h2)
                if 0.4 < asp < 2.0:
                    quads.append(approx)

        # ----- mask (ì—¬ëŸ¬ ì¢…ì´ union) -----
        mask = None

        if len(quads) > 0:
            # [ì„±ê³µ] ê°ì§€ë˜ì—ˆìœ¼ë¯€ë¡œ ì¹´ìš´í„° ë¦¬ì…‹
            missing_frame_count = 0 
            
            mask = np.zeros((h, w), np.uint8)
            for q in quads:
                cv2.fillConvexPoly(mask, q.reshape(4,2), 255)
        
        else:
            # [ì‹¤íŒ¨] ê°ì§€ ì•ˆ ë¨ -> ì¹´ìš´í„° ì¦ê°€
            missing_frame_count += 1
            
            # [íŒë‹¨] ìœ ì˜ˆ ê¸°ê°„(5í”„ë ˆì„)ì„ ë„˜ê²¼ëŠ”ê°€?
            if missing_frame_count > MAX_MISSING_FRAMES:
                last_mask = None
                last_overlay = None
            else:
                # 5í”„ë ˆì„ì´ ì•ˆ ì§€ë‚¬ìœ¼ë©´, maskëŠ” ì—†ì–´ë„(None)
                # last_overlayì™€ last_maskë¥¼ ì§€ìš°ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.
                pass

        # ----- OCR + ë²ˆì—­ (ì—¬ëŸ¬ ì¢…ì´) -----
        if mask is not None and (now - last_ocr_time) > OCR_INTERVAL:
            overlay_total = np.zeros_like(frame, dtype=np.uint8)
            all_texts = []

            for quad in quads:
                warped, M, src, dst = warp_quad(frame, quad)
                warped_gray = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)

                results = reader.readtext(warped_gray, detail=1)

                clusters = group_words(results)

                # ì´ ì¢…ì´ í•œ ì¥ì— ëŒ€í•œ ìƒíƒœ
                local_states = {}
                state_idx = 0

                for cluster in clusters:
                    full_eng = " ".join([word[3] for word in cluster]).strip()
                    if not full_eng:
                        continue

                    all_texts.append(full_eng)

                    # ë¬¸ì¥ bbox
                    pts_all = []
                    for word in cluster:
                        pts_all.extend(word[2])
                    pts_all = np.array(pts_all)

                    bbox_sentence = [
                        [int(np.min(pts_all[:,0])), int(np.min(pts_all[:,1]))],
                        [int(np.max(pts_all[:,0])), int(np.min(pts_all[:,1]))],
                        [int(np.max(pts_all[:,0])), int(np.max(pts_all[:,1]))],
                        [int(np.min(pts_all[:,0])), int(np.max(pts_all[:,1]))]
                    ]

                    # ë²ˆì—­ ìºì‹œ ì¡°íšŒ / ìš”ì²­
                    kor = translation_cache.get(full_eng)
                    if kor is None:
                        kor = "(ë²ˆì—­ ëŒ€ê¸°ì¤‘)"
                        if full_eng not in requested_texts:
                            translate_queue.put(full_eng)
                            requested_texts.add(full_eng)

                    state = {
                        "eng": full_eng,
                        "kor": kor,
                        "bbox": bbox_sentence
                    }
                    local_states[state_idx] = state
                    state_idx += 1

                # overlay ìƒì„± (warp ê³µê°„)
                if len(local_states) > 0:
                    warp_h, warp_w = warped.shape[:2]
                    # [ìˆ˜ì •] warped ì´ë¯¸ì§€ë¥¼ ì¸ìë¡œ ì „ë‹¬
                    warped_overlay = create_overlay(warp_h, warp_w, local_states, warped)

                    # warp-back
                    Minv = cv2.getPerspectiveTransform(dst, src)
                    ov = cv2.warpPerspective(warped_overlay, Minv, (w, h))

                    # ì—¬ëŸ¬ ì¢…ì´ overlayë¥¼ í•©ì„± (ìµœëŒ“ê°’ ì‚¬ìš©)
                    overlay_total = np.maximum(overlay_total, ov)

            if all_texts:
                print("[OCR]", " | ".join(all_texts))
            else:
                print("[OCR] (no text)")

            last_overlay = overlay_total
            last_mask = mask
            last_ocr_time = now

        # ----- ë°˜íˆ¬ëª… í•©ì„± -----
        if last_overlay is not None and last_mask is not None:
            mask_f = (last_mask.astype(np.float32)/255.0)[...,None]
            out_f = output.astype(np.float32)
            ov_f = last_overlay.astype(np.float32)

            # ALPHA = 0.3 (30%) ë¶ˆíˆ¬ëª…ë„ë¡œ ì›ë³¸ ì˜ìƒê³¼ AR ì˜¤ë²„ë ˆì´ í•©ì„±
            output = out_f*(1 - mask_f*ALPHA) + ov_f*(mask_f*ALPHA)
            output = output.astype(np.uint8)

        # ìœ¤ê³½ì„  ë””ë²„ê¹… (ëª¨ë“  ì¢…ì´)
        for q in quads:
            cv2.drawContours(output, [q], -1, (0,255,0), 3)

        cv2.imshow("RealSense AR Translate", output)
        k = cv2.waitKey(1)&0xFF
        if k in [27, ord('q')]:
            break
        
        if k == ord('s'):
            # ë©”ì¸ ì˜ìƒì„ ì ì‹œ ë©ˆì¶”ê³  í˜„ì¬ í”„ë ˆì„ ë³µì‚¬
            frame_to_process = frame.copy() 
            roi_thread = threading.Thread(
                target=select_and_translate_roi, 
                args=(frame_to_process, reader), 
                daemon=True # ë©”ì¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ì‹œ í•¨ê»˜ ì¢…ë£Œë˜ë„ë¡ ì„¤ì •
            )
            roi_thread.start()

    pipeline.stop()
    cv2.destroyAllWindows()


if __name__=="__main__":
    main()